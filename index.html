<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Mathematical Text</title>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap" rel="stylesheet">
    <style>
      body {
        margin: 0;
        min-height: 100vh;
        display: flex;
        justify-content: center;
        align-items: center;
        font-family: 'Crimson Text', Georgia, serif;
        line-height: 2;
        background: #363636;
        background-image: url('data:image/svg+xml,%3Csvg viewBox="0 0 200 200" xmlns="http://www.w3.org/2000/svg"%3E%3Cfilter id="noiseFilter"%3E%3CfeTurbulence type="fractalNoise" baseFrequency="1.5" numOctaves="4" stitchTiles="stitch"/%3E%3C/filter%3E%3Crect width="100%" height="100%" filter="url(%23noiseFilter)" opacity="0.08"/%3E%3C/svg%3E');
        color: #1a1a1a;
        font-size: 1.25rem;
      }
      .container {
        max-width: 900px;
        padding: 3rem;
        background: #f4f1ea;
        box-shadow: 0 8px 16px rgba(0, 0, 0, 0.1);
        border-radius: 16px;
        margin: 2rem;
      }
      .math-section {
        margin: 1.5rem 0;
        line-height: 2;
      }
      .equation {
        margin: 1.5rem 0;
        padding: 1rem 1.5rem;
        background: rgba(255, 255, 255, 0.7);
        border: 1px solid rgba(0, 0, 0, 0.1);
        border-radius: 8px;
        font-family: "Monaco", "Consolas", monospace;
        font-size: 1.15rem;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
      }
      .footer {
        margin-top: 3rem;
        padding-top: 1.5rem;
        border-top: 1px solid rgba(0, 0, 0, 0.1);
        font-size: 0.95em;
        color: #4a4a4a;
      }
      p {
        margin: 1.2rem 0;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="math-section">
        <p>Ah, but i think i disagree:</p>
        <p>
          The only problem is.. tightly knit nuclear families and strong
          religious beliefs are very low (Information theoretical-) entropy
          environments, which limits cognitive development.
        </p>

        <p>Let me prove this beyond reasonable doubt..:</p>

        <p>
          If we start with the KL (Kullback-Leibler) divergence between
          distributions Q and P: respectively probability distribution of policy
          Ï€ given state s_t under distribution Q; and the probability
          distribution of outcome (observation) o_t given state s_t under
          distribution P..
        </p>

        <div class="equation">
          KL(Q || P) = Î£_n E_q(s_t)[ Q(Ï€ | s_t) * ln(Q(Ï€ | s_t) / P(o_t | s_t))
          ]
        </div>

        <p>
          (which is the deciding factor in the variational free energy term)
        </p>

        <p>
          And we must consider that ğ›¼ may be asymmetrical, since the term Q(Ï€ |
          s_t) * ln(Q(Ï€ | s_t)) - representing the agent's internal uncertainty
          - is only fully determined by policy space and state when ğ›¼ approaches
          zero. This non-zero case is particularly revealing, as it exposes
          fundamental properties about variational free energy that cannot be
          simplified easily.
        </p>

        <p>To start, alpha:</p>

        <div class="equation">
          ğ›¼ = E_q(s_t)[ H(Q(Ï€|s_t)) ] / E_q(s_t,Ï€)[ H(P(o_t|s_t)) ]
        </div>

        <p>first, we fill in the Shannon entropy..</p>

        <div class="equation">
          H(Q(Ï€|s_t)) = - Î£_Ï€ Q(Ï€|s_t) * ln(Q(Ï€|s_t))<br />
          H(P(o_t|s_t)) = - Î£_o P(o_t|s_t) * ln(P(o_t|s_t))
        </div>

        <p>then we take the KL divergence..:</p>

        <div class="equation">
          KL(Q || P) = Î£_n E_q(s_t)[ Q(Ï€|s_t) * ln(Q(Ï€|s_t) / P(o_t|s_t)) ]
        </div>

        <p>
          and expand our ln(Q / P) term (dropping our assumption of symmetrical
          ğ›¼) ..
        </p>

        <div class="equation">
          = Î£_n [ E_q(s_t)[ Q(Ï€|s_t) * ln(Q(Ï€|s_t)) ] - E_q(s_t)[ Q(Ï€|s_t) *
          ln(P(o_t|s_t)) ] ]
        </div>

        <p>.. to relate KL to alpha. Notice:</p>

        <div class="equation">
          E_q(s_t)[ Q(Ï€|s_t) * ln(Q(Ï€|s_t))] = - E_q(s_t)[ H(Q(Ï€|s_t)) ]
        </div>

        <p>Also note that:</p>

        <div class="equation">
          E_q(s_t)[ H(Q(Ï€|s_t)) ] = ğ›¼ * E_q(s_t,Ï€)[H(P(o_t|s_t)) ]
        </div>

        <p>So we can rewrite KL(Q||P) using alpha:</p>

        <div class="equation">
          KL(Q || P) = - E_q(s_t)[ H(Q(Ï€|s_t)) ] - E_q(s_t)[ Q(Ï€|s_t) *
          ln(P(o_t|s_t)) ]
        </div>

        <p>And by substituting:</p>

        <div class="equation">
          E_q(s_t)[ H(Q(Ï€|s_t)) ] = ğ›¼ * E_q(s_t,Ï€)[ H(P(o_t|s_t)) ],
        </div>

        <p>we get:</p>

        <div class="equation">
          KL(Q || P) = -ğ›¼ * E_q(s_t,Ï€)[ H(P(o_t|s_t)) ] - E_q(s_t)[ Q(Ï€|s_t) *
          ln(P(o_t|s_t)) ].
        </div>

        <p>
          and we are done with this part.. let's look at alpha from another
          perspective:
        </p>

        <p>
          Let's.. examine one straightforward way to define a "distribution" of
          ğ›¼ values across states (or time).: if ğ›¼ is treated as a ratio of local
          entropies, suppose each state s has probability Q(s), and define..
        </p>

        <div class="equation">ğ›¼(s) = H(P(o|s)) / H(Q(Ï€|s))</div>

        <p>
          ..assuming H() - the Shannon entropy - stays positive, then ğ›¼ becomes
          a random variable over states, and Its CDF F_ğ›¼(ğ›¼) at any ğ›¼ â‰¥ 0 is:
        </p>

        <div class="equation">F_ğ›¼(a) = âˆ«[ ğ›¼(s) â‰¤ a ] Q(s) ds</div>

        <p>
          Or..: we integrate the probability mass Q(s) over all states s for
          which ğ›¼(s) â‰¤ a, which gives us the cumulative distribution for ğ›¼.
          Formally:
        </p>

        <div class="equation">
          F_ğ›¼(a) = P( ğ›¼(s) â‰¤ a ) = âˆ« 1{ ğ›¼(s) â‰¤ a } Q(s) ds
        </div>

        <p>
          This means we now have a distribution of ğ›¼(s) values across states,
          and the lower the area under the distribution, the less policy choice
          the agent has available.
        </p>

        <p>To show this, let's look at the expected entropy ratio:</p>

        <div class="equation">
          E_q[ ğ›¼ ] = âˆ« ğ›¼(s) * Q(s) ds = âˆ« [ H(P(o | s)) / H(Q(Ï€ | s)) ] * Q(s)
          ds
        </div>

        <p>
          I put forward the interpretation that this formulation, which is not
          reliant on a balanced ğ›¼, is especially important: we can deduce
          whether an agent has any policy choice available in the first place!
        </p>

        <p>
          Of course.. we cannot access the latent entropy distributions of an
          agent, H(P(o | s)), and H(Q(Ï€ | s)), but we can approximate them.
        </p>

        <p>
          First, we quickly show (it should be intuitive) that the distributions
          P and Q are solely dependent on the time series of observations.
          Recall..
        </p>

        <div class="equation">ğ›¼(s) = H(P(o|s)) / H(Q(Ï€|s))</div>

        <p>
          ..from above; here, s is a hidden state, Ï€ is a policy variable, and o
          is an observation.
        </p>

        <p>
          So, both numerator and denominator depend on the generative model and
          posterior (reminder: we were describing a models, a minds latent
          entropy distributions in the above section) which are fit using the
          time series {o_t}..:
        </p>

        <p>
          - Q(Ï€|s) is an internal (posterior/policy) distribution that the agent
          computes based on its model of how states relate to observations.<br /><br />
          - P(o|s) is the generative model for observations given states.
        </p>

        <p>
          Both are learned or defined in reference to the complete observation
          sequence: The agent's posterior Q(Ï€|s) and generative model P(o|s)
          only make sense within the context of how the agent infers or predicts
          observations over time. Hence, for a given hidden state s, these
          distributions, Q(Ï€|s) and P(o|s), are already conditioned (directly or
          indirectly) on the entire observed data {o_t}.
        </p>

        <p>
          (...In practice, the hidden state s or the policy Ï€ are not arbitrary
          â€” they are embedded in the latent state of a 'model' that was
          'trained'/updated using all past and possibly ongoing observations.)
        </p>

        <p>
          Therefore, the shape of the random variable ğ›¼ (its PDF/CDF) is
          entirely determined by the time series {o_t} (through the agent's
          posterior and generative model). Even though ğ›¼ is expressed in terms
          of s and Ï€, those variablesâ€”and their distributionsâ€”are internally
          derived from {o_t}.
        </p>

        <p>
          Disclaimer: Within the model, this determinacy holds; however, from an
          external standpoint, both distributions can only be approximated,
          making it locally indeterminate.
        </p>

        <p>
          Here, please take a break and remind yourself of the expected entropy
          ratio, and understand that we have now proven empirically, that, if
          active inference holds, the policy choice available to an agent is
          entirely dependent on the environment the agent is interacting with.
          Let it be clear, that latent policy execution - e.g. reasoning - is
          also bound by this. We have thus also "formulated epistemic
          injustice".
        </p>

        <p>
          Second, we show that the KL divergence, and therefore ğ›¼ (as well as
          P(o|s) and Q(Ï€|s)), of agents who share an environment can and indeed
          does converge through mutual observation and individual free energy
          minimization
        </p>

        <p>
          We start from KL(Q || P) in the single-agent setting, and we
          substitute the expression for ğ›¼ (.. = E_q(s_t)[ H(Q(Ï€|s_t))] /
          E_q(s_t,Ï€)[ H(P(o_t | s_t))] ..), and use the fact that E_q(s_t)[
          Q(Ï€|s_t) * ln(Q(Ï€|s_t)) ] is âˆ’E_q(s_t)[ H(Q(Ï€|s_t)) ].
        </p>

        <p>
          This yields the two main terms: one scaling with âˆ’ğ›¼ times the expected
          outcome entropy, and one depending on how much the agent's policy
          distribution converges or diverges from generative model P:
        </p>

        <div class="equation">
          KL_agent = - ğ›¼ * E_q(s_t, Ï€)[ H(P(o_t | s_t)) ] - E_q(s_t)[ Q(Ï€ | s_t)
          * ln(P(o_t | s_t)) ]
        </div>

        <p>
          Let us now define a "cluster-level" KL divergence. Assuming each agent
          n has its own generative model P_n and posterior Q_n, we can sum
          individual KL divergences (and optionally normalize by number of
          agents):
        </p>

        <div class="equation">
          KL_cluster = Î£_n[ - ğ›¼_n * E_q_n(s_t, Ï€)[ H(P_n(o_t | s_t)) ] -
          E_q_n(s_t)[ Q_n(Ï€ | s_t) * ln(P_n(o_t | s_t)) ] ]
        </div>

        <p>
          And finally, to establish a divergence between a single agent and its
          local cluster, we cross-compare agent a's distribution Q_a with each
          P_n in the cluster and sum over n â‰  a:
        </p>

        <div class="equation">
          KL_a_cluster = Î£_{nâ‰ a} [ - ğ›¼_a * E_q_a(s_t, Ï€)[ H(P_n(o_t | s_t)) ] -
          E_q_a(s_t)[ Q_a(Ï€ | s_t) * ln(P_n(o_t | s_t)) ] ]
        </div>

        <p>
          *Because agent a does not have direct access to agent n's internal
          generative model, ln(P_n) is an inference or approximation from the
          local perspective of a.
        </p>

        <p>
          This formulation captures how agent a's policy distribution diverges
          from every other agent n's generative model P_n.. and using that, we
          can begin laying out an argumentative proof:
        </p>

        <p>
          Premise: Each agent n (including agent a) performs free-energy
          minimization over time, iteratively updating Q_n(s_t) to minimize its
          own variational free energy F_n.
        </p>

        <div class="equation">
          F_n = KL( Q_n(s) || P_n(s|o_n) ) - ln(P_n(o_n))
        </div>

        <div class="equation">
          Minimizing F_n <=> Minimizing KL( Q_n(s) || P_n(s|o_n) )
        </div>

        <p>Attempted argument:</p>

        <p>
          1) All agents eventually converge to some "ensemble-consistent"
          posterior because each one updates Q_n(s) based on observations
          (including social/shared observations) and its priors.
        </p>

        <p>
          2) If the cluster is well-coupled (strongly overlapping observations),
          P_n(s | o_n) for each agent n evolves (not instantly, but
          progressively) toward a joint or ensemble distribution P_cluster.
        </p>

        <p>
          3) In the free-energy-minimizing limit, each Q_n*(s) will be close to
          P_cluster(s). This implies different Q_n*(s) distributions align with
          each other as well.
        </p>

        <p>
          4) Consequently, the KL divergence between any single agent's (Q_a,
          ğ›¼_a) and each P_n in the cluster drops over time:
        </p>

        <div class="equation">
          KL_a_cluster -> 0 as Q_a* -> P_cluster, P_n -> P_cluster.
        </div>

        <p>Therefore:</p>
        <div class="equation">
          ğ›¼_a * E_q_a[H(P_n(o_t|s_t))] and E_q_a[Q_a(Ï€|s_t) * ln(P_n(o_t|s_t))]
          both reach a minimum over time.
        </div>

        <p>
          Conclusion: Over time, each agent's posterior and policy distribution
          self-organizes around the shared cluster distribution. This reduces
          single-agent vs. cluster divergence, facilitating a collective low
          free-energy state.
        </p>

        <p>
          To bring this back to our original topic: we can now clearly see why
          increased segmentation (be it the nuclear family unit, class
          separation, caste systems..) of society results in increasingly
          stagnant flow of free energy and manifests class differences not just
          through a gradient of agency but even a gradient on maximum possible
          intelligence.
        </p>

        <p>
          Even further than this, these entropy gradients give one population an
          advantage over others - and, as potential is conserved, any
          proportionality delta between population sizes will multiply this
          advantage.
        </p>

        <p>Q.E.D.</p>

        <div class="footer">
          <p>
            Liu Grey Erotema<br />
            03:41 - 24.01.2025
          </p>

          <p>
            Citations:<br />
            Active Inference, MIT Press 2022, addendum .. B.2.5.. is where the
            equations I have (ab)used above are best explained
          </p>

          <p>
            Comment:<br />
            ..gosh I hope this is right, this caught me by surprise and I was
            having a TV and beer evening.. ''^^
          </p>
        </div>
      </div>
    </div>
  </body>
</html>
