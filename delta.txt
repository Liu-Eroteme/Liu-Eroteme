Ah, but i think i disagree:

The only problem is.. tightly knit nuclear families and strong religious beliefs are very low (Information theoretical-) entropy environments, which limits cognitive development.

Let me prove this beyond reasonable doubt..:

If we start with the KL (Kullback-Leibler) divergence between distributions Q and P: respectively probability distribution of policy Ï€ given state s_t under distribution Q;
and the probability distribution of outcome (observation) o_t given state s_t under distribution P..

KL(Q || P) = Î£_n E_q(s_t)[ Q(Ï€ | s_t) * ln(Q(Ï€ | s_t) / P(o_t | s_t)) ]

(which is the deciding factor in the variational free energy term)

And we must consider that ğ›¼ may be asymmetrical, since the term Q(Ï€ | s_t) * ln(Q(Ï€ | s_t)) - representing the agent's internal uncertainty - is only fully determined by policy space and state when ğ›¼ approaches zero. This non-zero case is particularly revealing, as it exposes fundamental properties about variational free energy that cannot be simplified easily.

To start, alpha:

ğ›¼ = E_q(s_t)[ H(Q(Ï€|s_t)) ] / E_q(s_t,Ï€)[ H(P(o_t|s_t)) ]
 
first, we fill in the Shannon entropy.. 

H(Q(Ï€|s_t)) = - Î£_Ï€ Q(Ï€|s_t) * ln(Q(Ï€|s_t))
H(P(o_t|s_t)) = - Î£_o P(o_t|s_t) * ln(P(o_t|s_t))

then we take the KL divergence..:

KL(Q || P) = Î£_n E_q(s_t)[ Q(Ï€|s_t) * ln(Q(Ï€|s_t) / P(o_t|s_t)) ]

and expand our ln(Q / P) term (dropping our assumption of symmetrical ğ›¼) ..

= Î£_n [ E_q(s_t)[ Q(Ï€|s_t) * ln(Q(Ï€|s_t)) ] - E_q(s_t)[ Q(Ï€|s_t) * ln(P(o_t|s_t)) ] ]

.. to relate KL to alpha. Notice:

E_q(s_t)[ Q(Ï€|s_t) * ln(Q(Ï€|s_t))] = - E_q(s_t)[ H(Q(Ï€|s_t)) ]

Also note that:

E_q(s_t)[ H(Q(Ï€|s_t)) ] = ğ›¼ * E_q(s_t,Ï€)[H(P(o_t|s_t)) ]

So we can rewrite KL(Q||P) using alpha:

KL(Q || P) = - E_q(s_t)[ H(Q(Ï€|s_t)) ] - E_q(s_t)[ Q(Ï€|s_t) * ln(P(o_t|s_t)) ]

And by substituting:

E_q(s_t)[ H(Q(Ï€|s_t)) ] = ğ›¼ * E_q(s_t,Ï€)[ H(P(o_t|s_t)) ],

we get:

KL(Q || P) = -ğ›¼ * E_q(s_t,Ï€)[ H(P(o_t|s_t)) ] - E_q(s_t)[ Q(Ï€|s_t) * ln(P(o_t|s_t)) ].

and we are done with this part.. let's look at alpha from another perspective:

Let's.. examine one straightforward way to define a "distribution" of ğ›¼ values across states (or time).: if ğ›¼ is treated as a ratio of local entropies, suppose each state s has probability Q(s), and define..

ğ›¼(s) = H(P(o|s)) / H(Q(Ï€|s))

..assuming H() - the Shannon entropy - stays positive, then ğ›¼ becomes a random variable over states, and Its CDF F_ğ›¼(ğ›¼) at any ğ›¼ â‰¥ 0 is:

F_ğ›¼(a) = âˆ«[ ğ›¼(s) â‰¤ a ] Q(s) ds

Or..: we integrate the probability mass Q(s) over all states s for which ğ›¼(s) â‰¤ a, which gives us the cumulative distribution for ğ›¼. Formally:

F_ğ›¼(a) = P( ğ›¼(s) â‰¤ a ) = âˆ« 1{ ğ›¼(s) â‰¤ a } Q(s) ds

This means we now have a distribution of ğ›¼(s) values across states, and the lower the area under the distribution, the less policy choice the agent has available.

To show this, let's look at the expected entropy ratio:

E_q[ ğ›¼ ] = âˆ« ğ›¼(s) * Q(s) ds = âˆ« [ H(P(o | s)) / H(Q(Ï€ | s)) ] * Q(s) ds

I put forward the interpretation that this formulation, which is not reliant on a balanced ğ›¼, is especially important: we can deduce whether an agent has any policy choice available in the first place!

Of course.. we cannot access the latent entropy distributions of an agent, H(P(o | s)), and H(Q(Ï€ | s)), but we can approximate them.

First, we quickly show (it should be intuitive) that the distributions P and Q are solely dependent on the time series of observations. Recall..

ğ›¼(s) = H(P(o|s)) / H(Q(Ï€|s))

..from above; here, s is a hidden state, Ï€ is a policy variable, and o is an observation.

So, both numerator and denominator depend on the generative model and posterior (reminder: we were describing a models, a minds latent entropy distributions in the above section) which are fit using the time series {o_t}..:

- Q(Ï€|s) is an internal (posterior/policy) distribution that the agent computes based on its model of how states relate to observations.

- P(o|s) is the generative model for observations given states.

Both are learned or defined in reference to the complete observation sequence: The agent's posterior Q(Ï€|s) and generative model P(o|s) only make sense within the context of how the agent infers or predicts observations over time.
Hence, for a given hidden state s, these distributions, Q(Ï€|s) and P(o|s), are already conditioned (directly or indirectly) on the entire observed data {o_t}.

(...In practice, the hidden state s or the policy Ï€ are not arbitrary â€” they are embedded in the latent state of a 'model' that was 'trained'/updated using all past and possibly ongoing observations.)

Therefore, the shape of the random variable ğ›¼ (its PDF/CDF) is entirely determined by the time series {o_t} (through the agent's posterior and generative model). Even though ğ›¼ is expressed in terms of s and Ï€, those variablesâ€”and their distributionsâ€”are internally derived from {o_t}.

Disclaimer: Within the model, this determinacy holds; however, from an external standpoint, both distributions can only be approximated, making it locally indeterminate.

Here, please take a break and remind yourself of the expected entropy ratio, and understand that we have now proven empirically, that, if active inference holds, the policy choice available to an agent is entirely dependent on the environment the agent is interacting with. Let it be clear, that latent policy execution - e.g. reasoning - is also bound by this. We have thus also "formulated epistemic injustice".

Second, we show that the KL divergence, and therefore ğ›¼ (as well as P(o|s) and Q(Ï€|s)), of agents who share an environment can and indeed does converge through mutual observation and individual free energy minimization

We start from KL(Q || P) in the single-agent setting, and we substitute the expression for ğ›¼ (.. = E_q(s_t)[ H(Q(Ï€|s_t))] / E_q(s_t,Ï€)[ H(P(o_t | s_t))] ..), and use the fact that E_q(s_t)[ Q(Ï€|s_t) * ln(Q(Ï€|s_t)) ] is âˆ’E_q(s_t)[ H(Q(Ï€|s_t)) ].

This yields the two main terms: one scaling with âˆ’ğ›¼ times the expected outcome entropy, and one depending on how much the agent's policy distribution converges or diverges from generative model P:

KL_agent = - ğ›¼ * E_q(s_t, Ï€)[ H(P(o_t | s_t)) ] - E_q(s_t)[ Q(Ï€ | s_t) * ln(P(o_t | s_t)) ]

Let us now define a "cluster-level" KL divergence. Assuming each agent n has its own generative model P_n and posterior Q_n, we can sum individual KL divergences (and optionally normalize by number of agents):

KL_cluster = Î£_n[ - ğ›¼_n * E_q_n(s_t, Ï€)[ H(P_n(o_t | s_t)) ] - E_q_n(s_t)[ Q_n(Ï€ | s_t) * ln(P_n(o_t | s_t)) ] ]

And finally, to establish a divergence between a single agent and its local cluster, we cross-compare agent a's distribution Q_a with each P_n in the cluster and sum over n â‰  a:

KL_a_cluster = Î£_{nâ‰ a} [ - ğ›¼_a * E_q_a(s_t, Ï€)[ H(P_n(o_t | s_t)) ] - E_q_a(s_t)[ Q_a(Ï€ | s_t) * ln(P_n(o_t | s_t)) ] ]

*Because agent a does not have direct access to agent n's internal generative model, ln(P_n) is an inference or approximation from the local perspective of a.

This formulation captures how agent a's policy distribution diverges from every other agent n's generative model P_n.. and using that, we can begin laying out an argumentative proof:

Premise: Each agent n (including agent a) performs free-energy minimization over time, iteratively updating Q_n(s_t) to minimize its own variational free energy F_n.

F_n = KL( Q_n(s) || P_n(s|o_n) ) - ln(P_n(o_n))

Minimizing F_n <=> Minimizing KL( Q_n(s) || P_n(s|o_n) )

Attempted argument:

1) All agents eventually converge to some "ensemble-consistent" posterior because each one updates Q_n(s) based on observations (including social/shared observations) and its priors.

2) If the cluster is well-coupled (strongly overlapping observations),
P_n(s | o_n) for each agent n evolves (not instantly, but progressively) toward a joint or ensemble distribution P_cluster.

3) In the free-energy-minimizing limit, each Q_n*(s) will be close to P_cluster(s). This implies different Q_n*(s) distributions align with each other as well.

4) Consequently, the KL divergence between any single agent's (Q_a, ğ›¼_a) and each P_n in the cluster drops over time:

KL_a_cluster -> 0  as  Q_a* -> P_cluster,  P_n -> P_cluster.

Therefore:
ğ›¼_a * E_q_a[H(P_n(o_t|s_t))] and E_q_a[Q_a(Ï€|s_t) * ln(P_n(o_t|s_t))] both reach a minimum over time.

Conclusion: Over time, each agent's posterior and policy distribution self-organizes around the shared cluster distribution. This reduces single-agent vs. cluster divergence, facilitating a collective low free-energy state.

To bring this back to our original topic: we can now clearly see why increased segmentation (be it the nuclear family unit, class separation, caste systems..) of society results in increasingly stagnant flow of free energy and manifests class differences not just through a gradient of agency but even a gradient on maximum possible intelligence.

Even further than this, these entropy gradients give one population an advantage over others - and, as potential is conserved, any proportionality delta between population sizes will multiply this advantage.


Q.E.D.



Liu Grey Erotema
03:41 - 24.01.2025



Citations:

Active Inference, MIT Press 2022, addendum .. B.2.5.. is where the equations I have (ab)used above are best explained


Comment:

..gosh I hope this is right, this caught me by surprise and I was having a TV and beer evening.. ''^^